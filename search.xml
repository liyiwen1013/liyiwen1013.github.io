<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Spark-RDD-实验</title>
      <link href="/2022/03/19/%E5%AE%9E%E9%AA%8C/"/>
      <url>/2022/03/19/%E5%AE%9E%E9%AA%8C/</url>
      
        <content type="html"><![CDATA[<h3 id="一、对spark有总体的认识，包括生态、架构、原理和特性等；"><a href="#一、对spark有总体的认识，包括生态、架构、原理和特性等；" class="headerlink" title="一、对spark有总体的认识，包括生态、架构、原理和特性等；"></a>一、对spark有总体的认识，包括生态、架构、原理和特性等；</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">txtFile = <span class="string">r&#x27;C:\Desktop\1.txt&#x27;</span></span><br><span class="line">rdd = sparkContext.textFile(txtFile)</span><br><span class="line">rdd.flatMap(<span class="keyword">lambda</span> x: x.split()).<span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>)).reduceByKey(<span class="keyword">lambda</span> x, y: x+y)</span><br></pre></td></tr></table></figure><h4 id="2、创建RDD"><a href="#2、创建RDD" class="headerlink" title="2、创建RDD"></a>2、创建RDD</h4><h5 id="（1）基于数据集合创建RDD"><a href="#（1）基于数据集合创建RDD" class="headerlink" title="（1）基于数据集合创建RDD"></a>（1）基于数据集合创建RDD</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">distData1 = sc.parallelize(data)   <span class="comment"># parallelize函数创建RDD</span></span><br><span class="line">distData2 = sc.parallelize(data, <span class="number">2</span>)  <span class="comment"># 指定分区数创建RDD，指定2个分区</span></span><br><span class="line">distData2.count() <span class="comment"># count函数返回RDD中元素的个数（不是某个分区的，是全部的元素个数）。</span></span><br></pre></td></tr></table></figure><p>遍历：对list、tuple、dict、set、str等类型的数据使用for…in…的循环语法从其中依次拿到数据进行使用，这样的过程称为遍历，也叫迭代。</p><p>可迭代对象（Iterable）：通过for…in…这类语句迭代读取一条数据供我们使用的对象。</p><h5 id="（2）基于外部数据源创建RDD"><a href="#（2）基于外部数据源创建RDD" class="headerlink" title="（2）基于外部数据源创建RDD"></a>（2）基于外部数据源创建RDD</h5><p>textFile 函数可以将一个外部数据源转换为RDD对象，文件的一行数据就是RDD的一个元素，需要传入该数据源的url，</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">distFile1 = sc.textFile(“file:///home/camel/Repos/spark/README.md”) <span class="comment"># 本地文件</span></span><br><span class="line">distFile2 = sc.textFile(“hdfs://地址：端口号/test/output”) <span class="comment"># hdfs上的文件</span></span><br><span class="line">distFile1.count() <span class="comment"># 此时的count函数返回的是文本的行数</span></span><br></pre></td></tr></table></figure><h4 id="3、RDD算子"><a href="#3、RDD算子" class="headerlink" title="3、RDD算子"></a>3、RDD算子</h4><p>作用于RDD上的Operation（操作）分为转换(transformantion)和动作(action)。</p><p>RDD拥有的操作比MR丰富的多，不仅仅包括Map、Reduce操作，还包括filter、sort、join、save、count等操作，所以Spark比MR更容易方便完成更复杂的任务。</p><h5 id="（1）转换操作-map"><a href="#（1）转换操作-map" class="headerlink" title="（1）转换操作 map"></a>（1）转换操作 map</h5><p>将指定的函数作用在RDD的每个分区的每个元素上</p><blockquote><p>rdd = sc.parallelize([“b”, “a”, “c”])<br>rdd.map(lambda x: (x, 1)).collect()<br>结果：<br>[(‘b’, 1), (‘a’, 1), (‘c’, 1)]</p></blockquote><p>map会依次取出rdd中的每一个元素，然后传给lambda x: (x, 1)中x变量，接着lambda针对x变量进行生成tuple (x,1)的操作，collect()是将各内存中的结果返回到驱动端，返回的是包含RDD所有元素的列表（list）</p><h5 id="（2）转换操作-flatMap"><a href="#（2）转换操作-flatMap" class="headerlink" title="（2）转换操作 flatMap"></a>（2）转换操作 flatMap</h5><p>首先将map函数应用于该RDD的所有元素，然后将结果平坦化（可以理解为将类型为元祖的元素逐个放出来），从而返回新的RDD</p><blockquote><p>rdd = sc.parallelize([2, 3, 4])<br>rdd.flatMap(lambda x: [(x, x), (x, x)]).collect()<br>[(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]</p><p>如果将flatMap换成map，结果则为：<br>[[(2, 2), (2, 2)], [(3, 3), (3, 3)], [(4, 4), (4, 4)]]</p></blockquote><h5 id="（3）转换操作-filter"><a href="#（3）转换操作-filter" class="headerlink" title="（3）转换操作 filter"></a>（3）转换操作 filter</h5><p>用指定的函数作为过滤条件，在RDD的所有分区中筛选出符合条件的元素</p><blockquote><p>rdd = sc.parallelize([1, 2, 3, 4, 5])<br>rdd.filter(lambda x: x % 2 == 0).collect()<br>[2, 4]</p></blockquote><h5 id="（4）转换操作-union"><a href="#（4）转换操作-union" class="headerlink" title="（4）转换操作 union"></a>（4）转换操作 union</h5><p>对一个RDD和参数RDD求并集后，返回一个新的RDD，只不过这里的union不会去重</p><blockquote><p>rdd1 = sc.parallelize([1, 2, 3, 4])<br>rdd2 = sc.parallelize([3, 4, 5, 6])<br>rdd1.union(rdd2).collect()<br>[1, 2, 3, 4, 3, 4, 5, 6]</p></blockquote><h5 id="（5）转换操作-intersection"><a href="#（5）转换操作-intersection" class="headerlink" title="（5）转换操作 intersection"></a>（5）转换操作 intersection</h5><p>对一个RDD和参数RDD求交集后，返回一个新的RDD</p><blockquote><p>rdd1 = sc.parallelize([1, 2, 3, 4])<br>rdd2 = sc.parallelize([3, 4, 5, 6])<br>rdd1. intersection(rdd2).collect()<br>[3, 4]</p></blockquote><h5 id="（6）转换操作-distinct"><a href="#（6）转换操作-distinct" class="headerlink" title="（6）转换操作 distinct"></a>（6）转换操作 distinct</h5><p>列表转成集合（set(a_list)）， distinct在RDD中返回包含不同元素（会去重）的新RDD</p><blockquote><p>rdd2 = sc.parallelize([3, 4, 5, 6, 3, 4])<br>rdd2.distinct().collect()<br>[3, 4, 5, 6]</p></blockquote><h5 id="（7）转换操作-sortBy"><a href="#（7）转换操作-sortBy" class="headerlink" title="（7）转换操作 sortBy"></a>（7）转换操作 sortBy</h5><p>对一个RDD和根据指定的key进行排序，返回一个新的RDD，默认是升序排列，如果要降序排列，则添加参数ascending=False</p><blockquote><p>tmp = [(“a”, 1), (“b”, 2), (“1”, 3), (“d”, 4), (“2”, 5)]<br>sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()<br>[(‘1’, 3), (‘2’, 5), (‘a’, 1), (‘b’, 2), (‘d’, 4)]<br>sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()<br>[(‘a’, 1), (‘b’, 2), (‘1’, 3), (‘d’, 4), (‘2’, 5)]<br>sc.parallelize(tmp).sortBy(lambda x: x[1], ascending=False).collect()<br>[(‘2’, 5), (‘d’, 4), (‘1’, 3), (‘b’, 2), (‘a’, 1)]</p></blockquote><h5 id="（8）转换操作-glom"><a href="#（8）转换操作-glom" class="headerlink" title="（8）转换操作 glom"></a>（8）转换操作 glom</h5><p>将原RDD中相同分区的元素合并到同一个列表中，合并形成的列表就是新RDD中的一个元素。通过glom转换，可以知道原RDD分区的情况</p><blockquote><p>data = [1, 2, 3, 4, 5, 6, 7, 8, 9]<br>rdd = sc.parallelize(data, 4)<br>rdd = rdd.glom()<br>rdd.collect()<br>[[1, 2], [3, 4], [5, 6], [7, 8, 9]]</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark-6</title>
      <link href="/2022/03/19/Spark-6/"/>
      <url>/2022/03/19/Spark-6/</url>
      
        <content type="html"><![CDATA[<h3 id="六、对图相关的概念要理解，掌握GraphFrames的使用以及实现的经典算法。"><a href="#六、对图相关的概念要理解，掌握GraphFrames的使用以及实现的经典算法。" class="headerlink" title="六、对图相关的概念要理解，掌握GraphFrames的使用以及实现的经典算法。"></a>六、对图相关的概念要理解，掌握GraphFrames的使用以及实现的经典算法。</h3><h4 id="1、图概念"><a href="#1、图概念" class="headerlink" title="1、图概念"></a>1、图概念</h4><p>在计算机科学中，图是一种重要的数据结构，它具有强大的表达能力，广泛应用于通信网络、搜索引擎、社交网络及自然语言处理等领域。</p><p>一般地，图(Graph)是由顶点的非空有限集和边的有限集构成的，记作G=<V,E>，其中G表示一个图，V表示图G中顶点(vertices)的集合，E表示是图G中边(edges)的集合，E中的边连接V中的两个顶点。</p><p>若E中的边没有方向，则用无序顶点对表示边，构成的图称为无向图；若E中的边有方向，则用有序顶点对来表示，构成的图成为有向图。</p><h4 id="2、图的度"><a href="#2、图的度" class="headerlink" title="2、图的度"></a>2、图的度</h4><blockquote><p>对于无向图，顶点的度是指连接该顶点的边的总和。<br>对于有向图，顶点的度分为出度(out-degree)和入度(in-degree)：<br>出度: 离开顶点的有向边的条数<br>入度: 进入该顶点的有向边的条数</p></blockquote><h4 id="3、图的路径和环"><a href="#3、图的路径和环" class="headerlink" title="3、图的路径和环"></a>3、图的路径和环</h4><blockquote><p>路径：一个连接两个不同顶点的序列v0e0…viej…ek-1vk，其中vi∈V,0&lt;i&lt;k;ej∈E,0&lt;j&lt;k-1,ej与vi，vi+1关联，且序列中的顶点各不相同。<br>环：起点和终点相同的路径就是环。<br>路径长度：路径长度为该路径上边的数目。<br>路径长度为1的环称为自环，即边的起点和终点为同一顶点。</p></blockquote><h4 id="4、连通分量"><a href="#4、连通分量" class="headerlink" title="4、连通分量"></a>4、连通分量</h4><blockquote><p>无向图G的极大连通子图称为G的连通分量。连通图的连通分量只有一个，即是其自身，非连通图有多个连通分量。<br>有向图G的极大强连通子图称为G的强连通分量，强连通图也只有一个强连通分量，即是其自身。非强连通的有向图有多个强连通分量。<br>极大连通子图一般称为连通分量</p></blockquote><p>网页排名算法 PageRank（Google左侧排名或佩奇排名）</p><p>原理：PageRank 通过网络的超链接关系来确定一个页面的等级。</p><p>pageRank函数的返回值为一个GraphFrame对象，在原GraphFrame对象的基础上，顶点df增加了pagerank列，为这个顶点的pagerank值，越大说明这个顶点就越重要。边表的df增加了weight列，为运行pageRank算法后该边的权重值，越高说明这条边就越重要。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark-5</title>
      <link href="/2022/03/19/Spark-5/"/>
      <url>/2022/03/19/Spark-5/</url>
      
        <content type="html"><![CDATA[<h3 id="五、掌握spark-在机器学习的应用，包括MLlib（简单了解）和ML（重点掌握），对机器学习的整个流程要理解，对ML的转换器、预测器和管道要理解并知道如何在实际例子中使用，掌握管道和模型保存和加载的方法，对模型的评估指标有大概的认识理解；"><a href="#五、掌握spark-在机器学习的应用，包括MLlib（简单了解）和ML（重点掌握），对机器学习的整个流程要理解，对ML的转换器、预测器和管道要理解并知道如何在实际例子中使用，掌握管道和模型保存和加载的方法，对模型的评估指标有大概的认识理解；" class="headerlink" title="五、掌握spark 在机器学习的应用，包括MLlib（简单了解）和ML（重点掌握），对机器学习的整个流程要理解，对ML的转换器、预测器和管道要理解并知道如何在实际例子中使用，掌握管道和模型保存和加载的方法，对模型的评估指标有大概的认识理解；"></a>五、掌握spark 在机器学习的应用，包括MLlib（简单了解）和ML（重点掌握），对机器学习的整个流程要理解，对ML的转换器、预测器和管道要理解并知道如何在实际例子中使用，掌握管道和模型保存和加载的方法，对模型的评估指标有大概的认识理解；</h3><h4 id="1、机器学习"><a href="#1、机器学习" class="headerlink" title="1、机器学习"></a>1、机器学习</h4><p>目前机器学习主流分为：监督学习，无监督学习，半监督学习，强化学习四种</p><p>人工智能可以看成一门学科，机器学习是实现人工智能的方法，而深度学习是实现机器学习的一门技术。</p><h4 id="2、数据的准备"><a href="#2、数据的准备" class="headerlink" title="2、数据的准备"></a>2、数据的准备</h4><p>数据准备包括数据的采集，数据的探索和数据的清洗。</p><blockquote><p>数据的采集一般来源于网络爬虫，企业的各种业务数据库，以及公开的数据源（如国家，公益组织等发布）。</p><p>数据的探索是对数据进行研究和判断，确定数据的质量与数据的特征，进而为下一步的数据预处理提供基本信息，数据探索环节与可视化展示连接紧密。</p><p>数据的清洗是对数据进行进一步的处理，包括数据的清洗、数据的转换、数据标准化、缺失值的处理、特征的提取、数据的降维等方面。</p></blockquote><h4 id="3、机器学习的流程"><a href="#3、机器学习的流程" class="headerlink" title="3、机器学习的流程"></a>3、机器学习的流程</h4><blockquote><p>机器学习的一般步骤是数据的采集，数据的加载，数据的探索，数据的预处理，训练模型，模型评估，模型的保存与调用。</p><p>Spark 机器学习库目前分为两个包，分别为mllib(pyspark.mllib)和ml(pyspark.ml)</p></blockquote><p>spark.mllib：基于RDD的机器学习API</p><p>spark.ml：基于DataFrames 高层次的API</p><h4 id="3、MLlib库的使用"><a href="#3、MLlib库的使用" class="headerlink" title="3、MLlib库的使用"></a>3、MLlib库的使用</h4><p>MLlib库主要操作的数据对象是RDD。</p><h4 id="4、ML库的使用"><a href="#4、ML库的使用" class="headerlink" title="4、ML库的使用"></a>4、ML库的使用</h4><p>ML库也是一个Spark机器学习库，ml主要操作的数据对象是DataFrame。</p><blockquote><p>ML包含三个主要的抽象类：Transformer（转换器），Estimator（预测器）和Pipline（管道）。</p></blockquote><p>Transformer：转换器，是一种可以将一个DataFrame转换为另一个DataFrame的算法。</p><blockquote><p>比如一个模型就是一个Transformer。它可以把一个不包含预测标签的测试数据集 DataFrame 打上标签，转化成另一个包含预测标签的 DataFrame。技术上，Transformer实现了一个transform（）方法，它通过附加一个或多个列将一个DataFrame转换为另一个DataFrame。</p></blockquote><p>Estimator：预测器，它是学习算法或在训练数据上的训练方法的概念抽象。</p><blockquote><p>在 Pipeline 里通常是被用来操作 DataFrame 数据并生产一个 Transformer。从技术上讲，Estimator实现了一个方法fit（），它接受一个DataFrame并产生一个转换器。如一个随机森林算法就是一个 Estimator，它可以调用fit（），通过训练特征数据而得到一个随机森林模型（模型是转换器）。</p></blockquote><p>PipeLine：翻译为工作流或者管道。</p><blockquote><p>工作流将多个工作流阶段（转换器和预测器）连接在一起，形成机器学习的工作流，并获得结果输出。</p></blockquote><p>Tokenizer，是转换器（Transformer），将原始DataFrame拆分为单词，并将带有单词的新列添加到DataFrame的words列。</p><p>HashingTF ，是转换器（Transformer），将words列转换为特征向量，并将带有特征向量的新列添加到DataFrame中.</p><h4 id="5、基本数据类型——本地向量"><a href="#5、基本数据类型——本地向量" class="headerlink" title="5、基本数据类型——本地向量"></a>5、基本数据类型——本地向量</h4><p>一个本地向量拥有从0开始的integer类型的索引以及float类型的值。</p><blockquote><p>MLlib和ML支持两种类型的本地向量：稠密(dense)向量和稀疏(sparse)向量。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark-4</title>
      <link href="/2022/03/19/Spark-4/"/>
      <url>/2022/03/19/Spark-4/</url>
      
        <content type="html"><![CDATA[<h2 id="四、掌握spark-streaming的工作原理、离散化流、实时数据获取（套接字和文件夹）等内容，掌握Dstream的各种转换（具体到代码的编写使用）；"><a href="#四、掌握spark-streaming的工作原理、离散化流、实时数据获取（套接字和文件夹）等内容，掌握Dstream的各种转换（具体到代码的编写使用）；" class="headerlink" title="四、掌握spark streaming的工作原理、离散化流、实时数据获取（套接字和文件夹）等内容，掌握Dstream的各种转换（具体到代码的编写使用）；"></a>四、掌握spark streaming的工作原理、离散化流、实时数据获取（套接字和文件夹）等内容，掌握Dstream的各种转换（具体到代码的编写使用）；</h2><h3 id="1、流数据"><a href="#1、流数据" class="headerlink" title="1、流数据"></a>1、流数据</h3><h4 id="（1）流数据的概念"><a href="#（1）流数据的概念" class="headerlink" title="（1）流数据的概念"></a>（1）流数据的概念</h4><p>流数据是一组顺序、大量、快速、连续到达的数据序列,一般情况下,数据流可被视为一个随时间延续而无限增长的动态数据集合。</p><h4 id="（2）流数据的特点"><a href="#（2）流数据的特点" class="headerlink" title="（2）流数据的特点"></a>（2）流数据的特点</h4><p>实时到达、次序独立、规模宏大、不易提取</p><h3 id="2、Spark-Streaming介绍"><a href="#2、Spark-Streaming介绍" class="headerlink" title="2、Spark Streaming介绍"></a>2、Spark Streaming介绍</h3><p>Spark Streaming 使得构建可扩展的容错流应用变得容易。它是Spark核心API的一个扩展，能够满足除对实时性要求非常高（如高频实时交易）之外的所有流式准实时计算场景</p><p>通过SparkStreaming处理后的结果还可以存储在数据库中，分布式文件系统HDFS中，还可以实时展现在数据大屏幕中等。</p><p>SparkStreaming能和机器学习库（MLlib）以及图计算库（Graphx）进行无缝衔接实现实时在线分析，以及使用DataFrame和SQL进行操作</p><h4 id="SparkStreaming特点："><a href="#SparkStreaming特点：" class="headerlink" title="SparkStreaming特点："></a>SparkStreaming特点：</h4><p>具有易于使用，高容错性，高吞吐量等特点使其能够胜任实时的流计算.</p><h3 id="3、SparkStreaming工作原理——分批次进行处理"><a href="#3、SparkStreaming工作原理——分批次进行处理" class="headerlink" title="3、SparkStreaming工作原理——分批次进行处理"></a>3、SparkStreaming工作原理——分批次进行处理</h3><blockquote><p>Spark Streaming接收实时输入数据流并将数据分成批处理，然后由Spark引擎处理以批量生成最终结果流。</p></blockquote><p><img src="https://gitee.com/liyiwen1013/picture-transmission/raw/master/img/image-20220105020015900.png" alt="image-20220105020015900"></p><p>先接收实时输入的数据流，然后将数据拆分成多个batch（批），比如每收集1秒的数据封装为一个batch，然后将每个batch交给Spark的计算引擎进行处理，最后会生产出一个结果数据流，结果中的数据也是由一个一个的batch所组成的。</p><h3 id="4、离散流"><a href="#4、离散流" class="headerlink" title="4、离散流"></a>4、离散流</h3><p>离散流（discretized stream）简称“Dstream”，这是Spark Streaming对内部持续的实时数据流的抽象描述，也就是我们处理的一个实时数据流，在Spark Streaming中对应于一个DStream 实例。</p><h4 id="（1）离散化流（Discretized-Stream）"><a href="#（1）离散化流（Discretized-Stream）" class="headerlink" title="（1）离散化流（Discretized Stream）"></a>（1）离散化流（Discretized Stream）</h4><blockquote><p>Discretized Stream或DStream是Spark Streaming提供的基本抽象。</p><p>它表示连续的数据流，可以是从源接收的输入数据流，也可以是通过转换输入流生成的已处理数据流。 在内部，DStream由一系列连续的RDD表示，这是Spark对不可变分布式数据集的抽象。</p><p>DStream中的每个RDD都包含来自特定时间间隔的数据。</p><p>DStram很好理解，就是按时间间隔组合的一系列RDD，其特点就是离散且持续。</p></blockquote><p><img src="https://gitee.com/liyiwen1013/picture-transmission/raw/master/img/image-20220105020209537.png" alt="image-20220105020209537"></p><h4 id="（2）使用套接字socket获取DStream"><a href="#（2）使用套接字socket获取DStream" class="headerlink" title="（2）使用套接字socket获取DStream"></a>（2）使用套接字socket获取DStream</h4><p>使用ssc对象的socketTextStream方法获取DStream</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line">sc = SparkContext(master=<span class="string">&quot;local[4]&quot;</span>) <span class="comment"># 指定以4个线程的本地模式运行Spark</span></span><br><span class="line">ssc = StreamingContext(sc, <span class="number">5</span>) <span class="comment"># 实例化一个StreamingContext对象，生成流数据的时候是每5秒一个批次</span></span><br><span class="line">lines = ssc.socketTextStream(<span class="string">&quot;10.2.87.2&quot;</span>, <span class="number">9999</span>) <span class="comment"># 从指定的ip和端口号获取数据</span></span><br><span class="line">counts = lines.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">&quot; &quot;</span>)).<span class="built_in">map</span>(<span class="keyword">lambda</span> word: (word, <span class="number">1</span>)).reduceByKey(<span class="keyword">lambda</span> a, b: a+b)</span><br><span class="line">counts.pprint() <span class="comment"># 统计各个时间段内每个单词出现的次数</span></span><br><span class="line">ssc.start() <span class="comment"># 启动流数据的执行</span></span><br><span class="line">ssc.awaitTermination() <span class="comment"># 等待执行停止</span></span><br></pre></td></tr></table></figure><h4 id="（3）使用文件系统中的文件获取DStream"><a href="#（3）使用文件系统中的文件获取DStream" class="headerlink" title="（3）使用文件系统中的文件获取DStream"></a>（3）使用文件系统中的文件获取DStream</h4><p>ssc对象的textFileStream方法</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sc = SparkContext(master=<span class="string">&quot;local[4]&quot;</span>)</span><br><span class="line">ssc = StreamingContext(sc, <span class="number">5</span>)</span><br><span class="line">ssc.checkpoint(<span class="string">r&quot;/home/ubuntu/test_checkpoit&quot;</span>)</span><br><span class="line">lines = ssc.socketTextStream(<span class="string">&quot;10.2.87.2&quot;</span>, <span class="number">9999</span>)</span><br><span class="line">counts.pprint()</span><br><span class="line">ssc.start()</span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></table></figure><p>文件夹里的文件必须是文本文件（txt,csv,json），且只有新增的文件才会被处理，在已经处理过的原有文件中添加新内容是无法获取到的，新增的文件最后修改日期也必须是最新的时间（修改时间大于程序的执行时间），且不能与已经处理过的文件同名，否则也无法获取。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark-3</title>
      <link href="/2022/03/19/Spark-3/"/>
      <url>/2022/03/19/Spark-3/</url>
      
        <content type="html"><![CDATA[<h3 id="一、对spark有总体的认识，包括生态、架构、原理和特性等；"><a href="#一、对spark有总体的认识，包括生态、架构、原理和特性等；" class="headerlink" title="一、对spark有总体的认识，包括生态、架构、原理和特性等；"></a>一、对spark有总体的认识，包括生态、架构、原理和特性等；</h3><h2 id="三、掌握对spark-dataframe和spark-sql的认识和使用（包括创建、各种常用操作，具体到代码的编写使用）；"><a href="#三、掌握对spark-dataframe和spark-sql的认识和使用（包括创建、各种常用操作，具体到代码的编写使用）；" class="headerlink" title="三、掌握对spark dataframe和spark sql的认识和使用（包括创建、各种常用操作，具体到代码的编写使用）；"></a>三、掌握对spark dataframe和spark sql的认识和使用（包括创建、各种常用操作，具体到代码的编写使用）；</h2><h3 id="1、DataFrame介绍"><a href="#1、DataFrame介绍" class="headerlink" title="1、DataFrame介绍"></a>1、DataFrame介绍</h3><p>在Spark中，Spark DataFrame和Spark SQL是SparkRDD高层次的封装，Spark DataFrame以RDD为基础，是一种与传统数据库中的二维表格相类似的分布式数据集。</p><p>DataFrame与RDD的主要区别：前者包含每一列的名称和类型</p><h3 id="2、创建Spark-DataFrame"><a href="#2、创建Spark-DataFrame" class="headerlink" title="2、创建Spark DataFrame"></a>2、创建Spark DataFrame</h3><h4 id="（1）通过本地数据结构-list或tuple-创建Spark-DataFrame"><a href="#（1）通过本地数据结构-list或tuple-创建Spark-DataFrame" class="headerlink" title="（1）通过本地数据结构(list或tuple)创建Spark DataFrame"></a>（1）通过本地数据结构(list或tuple)创建Spark DataFrame</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SQLContext</span><br><span class="line"><span class="comment"># 实例化一个SparkContext对象sc（如果是pyspark命令行，默认有一个sc对象，无需实例化sc）</span></span><br><span class="line">sc = SparkContext()</span><br><span class="line"><span class="comment"># 实例化一个SQLContext对象sqlContext ，需要将sc作为参数传入（如果是pyspark命令行，默认有一个sqlContext对象，无需实例化sqlContext ）</span></span><br><span class="line">sqlContext = SQLContext(sc)</span><br><span class="line">a_list = [(<span class="string">&quot;Jack&quot;</span>, <span class="number">20</span>, <span class="string">&quot;male&quot;</span>), (<span class="string">&quot;Rose&quot;</span>, <span class="number">18</span>, <span class="string">&quot;female&quot;</span>), (<span class="string">&quot;Tom&quot;</span>, <span class="number">19</span>, <span class="string">&quot;male&quot;</span>)]</span><br><span class="line">df = sqlContext. createDataFrame (a_list, [“name”, “age”, “gender”]) <span class="comment"># 第一个参数为序列对象，第二个参数指定列名</span></span><br><span class="line">df.show()  <span class="comment"># show方法展示当前Spark DataFrame的数据</span></span><br></pre></td></tr></table></figure><h4 id="（2）通过RDD创建Spark-DataFrame"><a href="#（2）通过RDD创建Spark-DataFrame" class="headerlink" title="（2）通过RDD创建Spark DataFrame"></a>（2）通过RDD创建Spark DataFrame</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SQLContext</span><br><span class="line">sc = SparkContext()</span><br><span class="line">sqlContext = SQLContext(sc)</span><br><span class="line">a_list = [(<span class="string">&quot;Jack&quot;</span>, <span class="number">20</span>, <span class="string">&quot;male&quot;</span>), (<span class="string">&quot;Rose&quot;</span>, <span class="number">18</span>, <span class="string">&quot;female&quot;</span>), (<span class="string">&quot;Tom&quot;</span>, <span class="number">19</span>, <span class="string">&quot;male&quot;</span>)]</span><br><span class="line">rdd = sc.parallelize(a_list)</span><br><span class="line">df = sqlContext.createDataFrame(rdd, [<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>, <span class="string">&quot;gender&quot;</span>])  <span class="comment"># 第一个参数为RDD对象，第二个参数指定列名</span></span><br><span class="line">df.show()  <span class="comment"># show方法展示当前Spark DataFrame的数据</span></span><br></pre></td></tr></table></figure><h4 id="（3）通过Pandas-DataFrame创建Spark-DataFrame"><a href="#（3）通过Pandas-DataFrame创建Spark-DataFrame" class="headerlink" title="（3）通过Pandas DataFrame创建Spark DataFrame"></a>（3）通过Pandas DataFrame创建Spark DataFrame</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SQLContext</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">pd_df = pd.read_csv(<span class="string">&quot;student_info.csv&quot;</span>)</span><br><span class="line">sc = SparkContext()</span><br><span class="line">sqlContext = SQLContext(sc)</span><br><span class="line">df = sqlContext.createDataFrame(pd_df)  <span class="comment"># pandas dataframe已经是行列结构的数据，自带列名，不需要再设置列名，第二个参数若传则有，不传则默认</span></span><br><span class="line">df.show()  <span class="comment"># show方法展示当前Spark DataFrame的数据</span></span><br></pre></td></tr></table></figure><p>注意： createDataFrame方法的参数，请参考上一页PPT。第二个参数不传的话，如果该数据源有列名，就会用该列名，否则使用spark的默认列名。如果传第二个参数，则不管数据源中有没有列名，都以传入的参数为列名。</p><h4 id="（4）通过csv文件创建Spark-DataFrame"><a href="#（4）通过csv文件创建Spark-DataFrame" class="headerlink" title="（4）通过csv文件创建Spark DataFrame"></a>（4）通过csv文件创建Spark DataFrame</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sc = SparkContext()</span><br><span class="line">sqlContext = SQLContext(sc)</span><br><span class="line">df_reader = sqlContext.read  <span class="comment"># sqlContext的read属性可以获取一个DataFrameReader对象</span></span><br><span class="line">df = df_reader.schema(<span class="string">&quot;name String, age Int, gender String&quot;</span>).csv(path=<span class="string">&quot;student_info.csv&quot;</span>)  <span class="comment"># 路径可以是hdfs路径</span></span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure><p>注意：上述程序中对csv文件的格式是有要求的，要求表头不带列名——因为程序中的红色部分已经声明表头了，csv文件中再有表头，会造成第一行全是null值</p><h4 id="（5）通过txt文件创建Spark-DataFrame"><a href="#（5）通过txt文件创建Spark-DataFrame" class="headerlink" title="（5）通过txt文件创建Spark DataFrame"></a>（5）通过txt文件创建Spark DataFrame</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sc = SparkContext()</span><br><span class="line">sqlContext = SQLContext(sc)</span><br><span class="line">df_reader = sqlContext.read  <span class="comment"># sqlContext的read属性可以获取一个DataFrameReader对象</span></span><br><span class="line">df = df_reader.schema(“name String”).text(paths=“names.txt”) <span class="comment"># paths可以是一个文件，也可以是一个文件夹</span></span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure><p>注意：txt文件的格式是有要求的，要求每一行只能是一个字段的数据，并且该字段的类型只能为String类型。</p><h4 id="（6）通过json文件创建Spark-DataFrame"><a href="#（6）通过json文件创建Spark-DataFrame" class="headerlink" title="（6）通过json文件创建Spark DataFrame"></a>（6）通过json文件创建Spark DataFrame</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sc = SparkContext()</span><br><span class="line">sqlContext = SQLContext(sc)</span><br><span class="line">df_reader = sqlContext.read  <span class="comment"># sqlContext的read属性可以获取一个DataFrameReader对象</span></span><br><span class="line">df = df_reader.json(path=<span class="string">&quot;student_info.json&quot;</span>)</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure><h4 id="（7）通过parquet文件创建Spark-DataFrame"><a href="#（7）通过parquet文件创建Spark-DataFrame" class="headerlink" title="（7）通过parquet文件创建Spark DataFrame"></a>（7）通过parquet文件创建Spark DataFrame</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sc = SparkContext()</span><br><span class="line">sqlContext = SQLContext(sc)</span><br><span class="line">df_reader = sqlContext.read  <span class="comment"># sqlContext的read属性可以获取一个DataFrameReader对象</span></span><br><span class="line">df = df_reader. parquet(paths=“user_info”)  <span class="comment">#  注意这是一个文件夹，里面有多个parquet文件</span></span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure><h4 id="（8）通过Hive数据表创建Spark-DataFrame"><a href="#（8）通过Hive数据表创建Spark-DataFrame" class="headerlink" title="（8）通过Hive数据表创建Spark DataFrame"></a>（8）通过Hive数据表创建Spark DataFrame</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sc = SparkContext()</span><br><span class="line">sqlContext = SQLContext(sc)</span><br><span class="line">df = sqlContext.table(“test_table<span class="string">&quot;)</span></span><br><span class="line"><span class="string">df.show()</span></span><br></pre></td></tr></table></figure><h4 id="（9）通过mysql数据库表创建Spark-DataFrame"><a href="#（9）通过mysql数据库表创建Spark-DataFrame" class="headerlink" title="（9）通过mysql数据库表创建Spark DataFrame"></a>（9）通过mysql数据库表创建Spark DataFrame</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sc = SparkContext()</span><br><span class="line">sqlContext = SQLContext(sc)</span><br><span class="line">df_reader = sqlContext.read  <span class="comment"># sqlContext的read属性可以获取一个DataFrameReader对象</span></span><br><span class="line"><span class="comment"># url设置数据库地址</span></span><br><span class="line"><span class="comment"># table设置要查询的数据库表</span></span><br><span class="line"><span class="comment"># properties 设置连接参数，是一个字典</span></span><br><span class="line">df = df_reader.jdbc(url=<span class="string">&quot;jdbc:mysql://127.0.0.1:3306/student&quot;</span>, table=<span class="string">&quot;student_info&quot;</span>,</span><br><span class="line">                    properties=<span class="built_in">dict</span>(user=<span class="string">&quot;root&quot;</span>, password=<span class="string">&quot;root&quot;</span>, driver=<span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>))</span><br><span class="line">df.show(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><h3 id="3、DataFrame的常用操作（函数-方法）"><a href="#3、DataFrame的常用操作（函数-方法）" class="headerlink" title="3、DataFrame的常用操作（函数/方法）"></a>3、DataFrame的常用操作（函数/方法）</h3><h4 id="4、Spark-SQL操作示例"><a href="#4、Spark-SQL操作示例" class="headerlink" title="4、Spark SQL操作示例"></a>4、Spark SQL操作示例</h4><p><img src="https://gitee.com/liyiwen1013/picture-transmission/raw/master/img/image-20220105015141678.png" alt="image-20220105015141678"></p><h4 id="4、Spark-SQL与DataFrame的区别与联系"><a href="#4、Spark-SQL与DataFrame的区别与联系" class="headerlink" title="4、Spark SQL与DataFrame的区别与联系"></a>4、Spark SQL与DataFrame的区别与联系</h4><p>DataFrame是一个分布式的数据集合，它按行组织，每行包含一组列，每列都有一个名称和一个关联的类型。换句话说，这个分布式数据集合具有由Schema定义的结构。你可以将它视为关系数据库中的表，但在底层，它具有更丰富的优化。<br>Spark SQL 是由DataFrame派生出来的，使用Spark SQL 之前必须先创建DataFrame，再注册临时表，然后才能使用Spark SQL。<br>Spark DataFrame是Spark的核心基础类， Spark SQL 是对DataFrame的高级封装。<br>Spark SQL相比于Spark DataFrame更灵活易用。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark-2</title>
      <link href="/2022/03/19/Spark-2-RDD/"/>
      <url>/2022/03/19/Spark-2-RDD/</url>
      
        <content type="html"><![CDATA[<h3 id="一、对spark有总体的认识，包括生态、架构、原理和特性等；"><a href="#一、对spark有总体的认识，包括生态、架构、原理和特性等；" class="headerlink" title="一、对spark有总体的认识，包括生态、架构、原理和特性等；"></a>一、对spark有总体的认识，包括生态、架构、原理和特性等；</h3><h3 id="二、掌握spark-RDD的概念、算子的作用和使用（包括创建和各种转换运算，具体到代码的编写使用），不同共享变量的作用和使用、对于RDD的依赖关系要理解，知道持久化的方法以及类型；"><a href="#二、掌握spark-RDD的概念、算子的作用和使用（包括创建和各种转换运算，具体到代码的编写使用），不同共享变量的作用和使用、对于RDD的依赖关系要理解，知道持久化的方法以及类型；" class="headerlink" title="二、掌握spark RDD的概念、算子的作用和使用（包括创建和各种转换运算，具体到代码的编写使用），不同共享变量的作用和使用、对于RDD的依赖关系要理解，知道持久化的方法以及类型；"></a>二、掌握spark RDD的概念、算子的作用和使用（包括创建和各种转换运算，具体到代码的编写使用），不同共享变量的作用和使用、对于RDD的依赖关系要理解，知道持久化的方法以及类型；</h3><h4 id="1、Spark-RDD的概念"><a href="#1、Spark-RDD的概念" class="headerlink" title="1、Spark RDD的概念"></a>1、Spark RDD的概念</h4><blockquote><p>RDD是可扩展的弹性分布式数据集（一种容错的并行数据结构）；</p><p>是只读、分区且不变的数据集合；</p><p>是Spark的基石，也是Spark的灵魂；</p><p>是一种分布式的内存抽象，不具备Schema的数据结构（可以基于任何数据结构创建，如tuple（元组）、dict（字典）和list（列表））</p></blockquote><h5 id="RDD的五个主要属性："><a href="#RDD的五个主要属性：" class="headerlink" title="RDD的五个主要属性："></a>RDD的五个主要属性：</h5><blockquote><p>（1）分区信息（Partition)（2）自定义分片计算（3）RDD之间相互依赖（4）控制分片数量（5）使用列表方式进行块存储</p></blockquote><h4 id="4、共享变量"><a href="#4、共享变量" class="headerlink" title="4、共享变量"></a>4、共享变量</h4><h5 id="（1）累加器（Accumulator）"><a href="#（1）累加器（Accumulator）" class="headerlink" title="（1）累加器（Accumulator）"></a>（1）累加器（Accumulator）</h5><p>累加器是一个全局的共享变量，累加器可以很好地解决上述程序的闭包问题。使用累加器完成相同的功能，代码如下：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">sum</span> = sc.accumulator(<span class="number">0</span>)  <span class="comment"># 创建一个累加器，初值为0</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fn1</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">global</span> <span class="built_in">sum</span></span><br><span class="line">    <span class="built_in">sum</span> += x  <span class="comment"># 注意这里不能是 sum=sum+x，因为+=是原地操作，+是需要两个变量类型一致。</span></span><br><span class="line">a_rdd = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">a_rdd.foreach(fn1)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">sum</span>.value)  <span class="comment"># sum.value可以获取累加器的值，此时打印输出的是15</span></span><br></pre></td></tr></table></figure><p>累加器是一个write-only的变量，工作节点worker中的task无法读取这个值，只能在驱动程序中使用value方法来读取累加器的值。</p><h5 id="（2）广播变量（Broadcast）"><a href="#（2）广播变量（Broadcast）" class="headerlink" title="（2）广播变量（Broadcast）"></a>（2）广播变量（Broadcast）</h5><p>广播变量和累加器类似，也是一个共享变量，广播变量能够以一种更有效率的方式将一个大数据量输入集合的副本分配给每个节点。</p><p>SparkContext对象的broadcast方法可以创建广播变量，广播变量的value属性可以获取该广播变量的值，unpersist方法可以在执行程序上删除此广播的缓存副本。destroy方法可以销毁广播变量，一旦广播变量被销毁，就不能再使用了。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = sc.broadcast(<span class="number">10</span>)  <span class="comment"># 创建一个广播对象</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b.value  <span class="comment"># 获取广播对象的值</span></span><br><span class="line"><span class="number">10</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]).<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x * b.value).collect()</span><br><span class="line">[<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>, <span class="number">50</span>] </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b.destroy()  <span class="comment"># 销毁广播变量，销毁后就不能访问它的value了</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b.value  <span class="comment">#  但是pyspark中还是能访问到这个值，这是pyspark的问题，如果是scala确实是无法访问它的值了</span></span><br><span class="line"><span class="number">10</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]).<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x * b.value).collect()  <span class="comment"># task中确实无法访问该广播变量的值了</span></span><br></pre></td></tr></table></figure><h4 id="5、依赖问题"><a href="#5、依赖问题" class="headerlink" title="5、依赖问题"></a>5、依赖问题</h4><h5 id="（1）RDD只能基于在稳定物理存储中的数据集和其他已有的RDD上执行确定性操作来创建。"><a href="#（1）RDD只能基于在稳定物理存储中的数据集和其他已有的RDD上执行确定性操作来创建。" class="headerlink" title="（1）RDD只能基于在稳定物理存储中的数据集和其他已有的RDD上执行确定性操作来创建。"></a>（1）RDD只能基于在稳定物理存储中的数据集和其他已有的RDD上执行确定性操作来创建。</h5><h5 id="（2）RDD在血统依赖方面，分为窄依赖和宽依赖。他们用来解决数据容错的高效性。"><a href="#（2）RDD在血统依赖方面，分为窄依赖和宽依赖。他们用来解决数据容错的高效性。" class="headerlink" title="（2）RDD在血统依赖方面，分为窄依赖和宽依赖。他们用来解决数据容错的高效性。"></a>（2）RDD在血统依赖方面，分为窄依赖和宽依赖。他们用来解决数据容错的高效性。</h5><blockquote><p>窄依赖：<br>一个父RDD的分区partition最多被子RDD的一个分区使用（独生子女）。<br>在一个集群节点上管道式执行。<br>比如map、filter、union等；</p><p>宽依赖：<br>多个子RDD的Partition会依赖同一个父RDD的Partition，会引起shuffle（超生）。<br>比如groupByKey、reduceByKey、 sortBy、partitionBy等；</p></blockquote><p>注意：一个RDD对不同的父节点可能有不同的依赖方式，可能对父节点1是宽依赖，对父节点2是窄依赖。</p><blockquote><p>shuffle：Spark 里的某些操作会触发 shuffle，shuffle 是spark 重新分配数据的一种机制，使得这些数据可以跨不同的区域进行分组。</p><p>DAG：Spark里的每一个转换操作都会生成一个新的RDD，RDD之间连一条边，最后这些RDD和他们之间的边组成一个有向无环图DAG(Directed Acyclic Graph)。</p><p>一个Stage的开始就是从外部存储或者shuffle结果中读取数据；一个Stage的结束就是发生shuffle或者生成结果时。</p><p>spark划分stage的整体思路是：从后往前推，遇到宽依赖就断开，划分为一个stage；遇到窄依赖就将这个RDD加入该stage中。</p></blockquote><h4 id="6、RDD的持久化"><a href="#6、RDD的持久化" class="headerlink" title="6、RDD的持久化"></a>6、RDD的持久化</h4><blockquote><p>可以使用persist方法和cache方法，cache方法只能缓存在内存中， persist方法可以缓存在磁盘上或者内存中。</p></blockquote><p>is_cached属性可以查看当前RDD的持久化状态，或者使用getStorageLevel方法获取当前RDD的持久化状态，unpersist方法可以解除RDD的持久化</p><div class="table-container"><table><thead><tr><th><strong>StorageLevel**</strong>类型**</th><th><strong>类型描述</strong></th><th><strong>对应的**</strong>useDisk,<strong> </strong>useMemory, deserialized, off_heap, replication**</th></tr></thead><tbody><tr><td>MEMORY_ONLY</td><td>（默认级别）将RDD以JAVA对象的形式保存到JVM内存。如果分片太大，内存缓存不下，就不缓存</td><td>StorageLevel(False, True, False, False, 1)</td></tr><tr><td>MEMORY_ONLY_2</td><td>（默认级别）将RDD以JAVA对象的形式保存到JVM内存。如果分片太大，内存缓存不下，就不缓存，将分区复制到两个集群节点上</td><td>StorageLevel(False, True, False, False, 2)</td></tr><tr><td>MEMORY_ONLY_SER</td><td>将RDD以序列化的JAVA对象形式保存到内存</td><td>StorageLevel(False, True, False, False, 1)</td></tr><tr><td>MEMORY_ONLY_SER_2</td><td>将RDD以序列化的JAVA对象形式保存到内存，将分区复制到两个集群节点上</td><td>StorageLevel(False, True, False, False, 2)</td></tr><tr><td>DISK_ONLY</td><td>将RDD持久化到硬盘</td><td>StorageLevel(True, False, False, False, 1)</td></tr><tr><td>DISK_ONLY_2</td><td>将RDD持久化到硬盘，将分区复制到两个集群节点上</td><td>StorageLevel(True, False, False, False, 2)</td></tr><tr><td>MEMORY_AND_DISK</td><td>将RDD数据集以JAVA对象的形式保存到JVM内存中，如果分片太大不能保存到内存中，则保存到磁盘上，下次用时重新从磁盘读取</td><td>StorageLevel(True, True, False, False, 1)</td></tr><tr><td>MEMORY_AND_DISK_2</td><td>将RDD数据集以JAVA对象的形式保存到JVM内存中，如果分片太大不能保存到内存中，则保存到磁盘上，下次用时重新从磁盘读取，并将分区复制到两个集群节点上</td><td>StorageLevel(True, True, False, False, 2)</td></tr><tr><td>MEMORY_AND_DISK_SER</td><td>与MEMORY_ONLY_SER类似，但当分片太大，不能保存到内存中，会将其保存到磁盘中</td><td>StorageLevel(True, True, False, False, 1)</td></tr><tr><td>MEMORY_AND_DISK_SER_2</td><td>与MEMORY_ONLY_SER类似，但当分片太大，不能保存到内存中，会将其保存到磁盘中，将分区复制到两个集群节点上</td><td>StorageLevel(True, True, False, False, 2)</td></tr><tr><td>OFF_HEAP</td><td>是否利用java unsafe API实现的内存管理，RDD实际被保存到Tachyon</td><td>StorageLevel(True, True, True, False, 1)</td></tr></tbody></table></div><h5 id="检查点-checkpoint"><a href="#检查点-checkpoint" class="headerlink" title="检查点 checkpoint"></a>检查点 checkpoint</h5><p>通过cache或者persist将RDD持久化到内存或者磁盘中，这样做并不能保证数据完全不会丢失，当数据丢失的时候，Spark会根据RDD的计算流程DGA重新计算一遍，这样子就很费性能，</p><blockquote><p>checkpoint的作用就是将DAG中比较重要的中间数据做一个检查点将结果存储到一个高可用的地方(通常这个地方就是HDFS里面，当然也可以是本地文件系统)。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark-1-概念</title>
      <link href="/2022/03/19/Spark-1-%E6%A6%82%E5%BF%B5/"/>
      <url>/2022/03/19/Spark-1-%E6%A6%82%E5%BF%B5/</url>
      
        <content type="html"><![CDATA[<h3 id="一、对spark有总体的认识，包括生态、架构、原理和特性等；"><a href="#一、对spark有总体的认识，包括生态、架构、原理和特性等；" class="headerlink" title="一、对spark有总体的认识，包括生态、架构、原理和特性等；"></a>一、对spark有总体的认识，包括生态、架构、原理和特性等；</h3><h4 id="1、Spark的概念："><a href="#1、Spark的概念：" class="headerlink" title="1、Spark的概念："></a>1、Spark的概念：</h4><p>Spark 是一个类于 Hadoop MapReduce 的通用并行框架, 由Scala语言实现的专门为大规模数据处理而设计的快速通用的技术分析引擎。</p><p>Spark的特点：快速、通用、易用、兼容性好。</p><p>Spark具备的能力：机器学习（ML库），实时流计算（Spark Streaming），SQL查询统计（Spark SQL），图表计算（GraphFrame）。</p><h4 id="2、Spark与Hadoop的区别与联系："><a href="#2、Spark与Hadoop的区别与联系：" class="headerlink" title="2、Spark与Hadoop的区别与联系："></a>2、Spark与Hadoop的区别与联系：</h4><p>（1）解决问题的方式不一样（Hadoop存储和计算，Spark处理不存储）（2）两者可合可分（Hadoop独自美丽，Spark要依赖分布式文件系统）</p><h4 id="3、Spark相比HadoopMapRedue的优势："><a href="#3、Spark相比HadoopMapRedue的优势：" class="headerlink" title="3、Spark相比HadoopMapRedue的优势："></a>3、Spark相比HadoopMapRedue的优势：</h4><p>（1） 中间结果输出（MapReduce磁盘，Spark内存）（2）数据格式和内存布局（3）执行策略（4） 任务调度的开销</p><h4 id="4、Spark-的用途："><a href="#4、Spark-的用途：" class="headerlink" title="4、Spark 的用途："></a>4、Spark 的用途：</h4><p>（1）推荐系统（2）快速查询系统（3）实时日志处理（4）定制广告系统（5）用户图计算</p><h4 id="5、Spark的生态系统："><a href="#5、Spark的生态系统：" class="headerlink" title="5、Spark的生态系统："></a>5、Spark的生态系统：</h4><p><img src="https://gitee.com/liyiwen1013/picture-transmission/raw/master/img/image-20211228003837117.png" alt="image-20211228003837117"></p><p>以Spark Core 为核心，利用Standalone、YARN 和Mesos 等资源调度管理，完成应用程序分析与处理。</p><p>Spark Core提供Spark最基础与最核心的功能，它的子框架包括Spark SQL，Spark Streaming，MLlib和GraphX。</p><p>Spark SQL是一种结构化的数据处理模块。它提供了一个称为DataFrame的编程抽象，也可以作为分布式SQL查询引擎。</p><p>Spark Streaming是Sprak API核心的一个超高通量的扩展，可以处理实时数据流并容错。</p><p>Mllib是Spark提供的可扩展的机器学习库，包含了一些通用的学习算法和工具，如分类、回归、聚类、协同过滤、降维，以及底层的优化原语等算法和工具。</p><p>GraphX在Graphs和Graph-parallel并行计算中是一个新的部分，GraphX是Spark上的分布式图形处理架构，可用于图表计算。</p><h4 id="6、Spark-的构架"><a href="#6、Spark-的构架" class="headerlink" title="6、Spark 的构架"></a>6、Spark 的构架</h4><p><img src="https://gitee.com/liyiwen1013/picture-transmission/raw/master/img/image-20211228003137424.png" alt="image-20211228003137424"></p><p>Driver APP：客户端驱动程序或者应用程序，用于将任务程序转换为RDD和DAG，并与Cluster Manager进行通信与调度；<br>Cluster Manager：Spark的集群管理器，主要负责资源的分配和管理；<br>Worker：Spark的工作节点，对于Spark应用程序而言，由Cluster Manager分配资源给Worker，获得资源的Worker将创建Executor，并将资源和任务进一步分配给Executor，然后同步资源信息给Cluster Manager；<br>Executor：Spark的任务执行单元，负责任务的执行以及和Worker、Driver App同步信息。</p><h4 id="7、SparkContext核心原理——Spark分布式流计算"><a href="#7、SparkContext核心原理——Spark分布式流计算" class="headerlink" title="7、SparkContext核心原理——Spark分布式流计算"></a>7、SparkContext核心原理——Spark分布式流计算</h4><p><img src="https://gitee.com/liyiwen1013/picture-transmission/raw/master/img/image-20211228003519253.png" alt="image-20211228003519253"></p><p>(1) 从代码构建DAG图。<br>(2) 将DAG划分为Stage。<br>(3) Stage生成作业。<br>(4) FinalStage提交任务集。<br>(5) TaskSets提交任务。<br>(6) Tasks执行任务。<br>(7) Results跟踪结果。</p><h4 id="8、Spark-2-X"><a href="#8、Spark-2-X" class="headerlink" title="8、Spark 2.X"></a>8、Spark 2.X</h4><p>8.1 具有精简的API</p><p>（1）统一DataFrame和Dataset接口<br>（2）新增SparkSession接口<br>（3）为SparkSession提供全新的、工作流式配置<br>（4）提供更易用、更高效的计算接口<br>（5）Dataset中的聚合操作有全新的、改进的聚合接口</p><p>8.2 Spark作为编译器</p><p>其主要思想就是在运行时使用优化后的字节码。</p><p>8.3 智能化程度</p><h3 id="判断题："><a href="#判断题：" class="headerlink" title="判断题："></a>判断题：</h3><p>1、不仅仅Spark是基于内存的计算技术，Hadoop也是基于内存的计算，基本上所有的技术都基于内存进行计算。Spark只是把计算过程中间的结果缓存在内存中。<br>2、Spark只是在逻辑回归测试时候速度比Hadoop快了100倍，其他算法不一定。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark-RDD</title>
      <link href="/2021/12/29/Spark-RDD/"/>
      <url>/2021/12/29/Spark-RDD/</url>
      
        <content type="html"><![CDATA[<h4 id="1、Spark-RDD的概念"><a href="#1、Spark-RDD的概念" class="headerlink" title="1、Spark RDD的概念"></a>1、Spark RDD的概念</h4><p>RDD是可扩展的弹性分布式数据集（一种容错的并行数据结构）；是只读、分区且不变的数据集合；是Spark的基石，也是Spark的灵魂；是一种分布式的内存抽象，不具备Schema的数据结构（可以基于任何数据结构创建，如tuple（元组）、dict（字典）和list（列表））</p><p>RDD的五个主要属性：（1）分区信息（Partition)（2）自定义分片计算（3）RDD之间相互依赖（4）控制分片数量（5）使用列表方式进行块存储</p><h4 id="2、创建RDD"><a href="#2、创建RDD" class="headerlink" title="2、创建RDD"></a>2、创建RDD</h4><h5 id="（1）基于数据集合创建RDD"><a href="#（1）基于数据集合创建RDD" class="headerlink" title="（1）基于数据集合创建RDD"></a>（1）基于数据集合创建RDD</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">distData1 = sc.parallelize(data)   <span class="comment"># parallelize函数创建RDD</span></span><br><span class="line">distData2 = sc.parallelize(data, <span class="number">2</span>)  <span class="comment"># 指定分区数创建RDD，指定2个分区</span></span><br><span class="line">distData2.count() <span class="comment"># count函数返回RDD中元素的个数（不是某个分区的，是全部的元素个数）。</span></span><br></pre></td></tr></table></figure><p>遍历：对list、tuple、dict、set、str等类型的数据使用for…in…的循环语法从其中依次拿到数据进行使用，这样的过程称为遍历，也叫迭代。</p><p>可迭代对象（Iterable）：通过for…in…这类语句迭代读取一条数据供我们使用的对象。</p><h5 id="（2）基于外部数据源创建RDD"><a href="#（2）基于外部数据源创建RDD" class="headerlink" title="（2）基于外部数据源创建RDD"></a>（2）基于外部数据源创建RDD</h5><p>textFile 函数可以将一个外部数据源转换为RDD对象，文件的一行数据就是RDD的一个元素，需要传入该数据源的url，</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">distFile1 = sc.textFile(“file:///home/camel/Repos/spark/README.md”) <span class="comment"># 本地文件</span></span><br><span class="line">distFile2 = sc.textFile(“hdfs://地址：端口号/test/output”) <span class="comment"># hdfs上的文件</span></span><br><span class="line">distFile1.count() <span class="comment"># 此时的count函数返回的是文本的行数</span></span><br></pre></td></tr></table></figure><h4 id="3、RDD算子"><a href="#3、RDD算子" class="headerlink" title="3、RDD算子"></a>3、RDD算子</h4><p>作用于RDD上的Operation分为转换(transformantion)和动作(action)。</p><p>RDD拥有的操作比MR丰富的多，不仅仅包括Map、Reduce操作，还包括filter、sort、join、save、count等操作，所以Spark比MR更容易方便完成更复杂的任务。</p><h5 id="（1）转换操作-map"><a href="#（1）转换操作-map" class="headerlink" title="（1）转换操作 map"></a>（1）转换操作 map</h5><p>将指定的函数作用在RDD的每个分区的每个元素上</p><blockquote><p>rdd = sc.parallelize([“b”, “a”, “c”])<br>rdd.map(lambda x: (x, 1)).collect()<br>结果：<br>[(‘b’, 1), (‘a’, 1), (‘c’, 1)]</p></blockquote><p>map会依次取出rdd中的每一个元素，然后传给lambda x: (x, 1)中x变量，接着lambda针对x变量进行生成tuple (x,1)的操作，collect()是将各内存中的结果返回到驱动端，返回的是包含RDD所有元素的列表（list）</p><h5 id="（2）转换操作-flatMap"><a href="#（2）转换操作-flatMap" class="headerlink" title="（2）转换操作 flatMap"></a>（2）转换操作 flatMap</h5><p>首先将map函数应用于该RDD的所有元素，然后将结果平坦化（可以理解为将类型为元祖的元素逐个放出来），从而返回新的RDD</p><blockquote><p>rdd = sc.parallelize([2, 3, 4])<br>rdd.flatMap(lambda x: [(x, x), (x, x)]).collect()<br>[(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]</p><p>如果将flatMap换成map，结果则为：<br>[[(2, 2), (2, 2)], [(3, 3), (3, 3)], [(4, 4), (4, 4)]]</p></blockquote><h5 id="（3）转换操作-filter"><a href="#（3）转换操作-filter" class="headerlink" title="（3）转换操作 filter"></a>（3）转换操作 filter</h5><p>用指定的函数作为过滤条件，在RDD的所有分区中筛选出符合条件的元素</p><blockquote><p>rdd = sc.parallelize([1, 2, 3, 4, 5])<br>rdd.filter(lambda x: x % 2 == 0).collect()<br>[2, 4]</p></blockquote><h5 id="（4）转换操作-union"><a href="#（4）转换操作-union" class="headerlink" title="（4）转换操作 union"></a>（4）转换操作 union</h5><p>对一个RDD和参数RDD求并集后，返回一个新的RDD，只不过这里的union不会去重</p><blockquote><p>rdd1 = sc.parallelize([1, 2, 3, 4])<br>rdd2 = sc.parallelize([3, 4, 5, 6])<br>rdd1.union(rdd2).collect()<br>[1, 2, 3, 4, 3, 4, 5, 6]</p></blockquote><h5 id="（5）转换操作-intersection"><a href="#（5）转换操作-intersection" class="headerlink" title="（5）转换操作 intersection"></a>（5）转换操作 intersection</h5><p>对一个RDD和参数RDD求交集后，返回一个新的RDD</p><blockquote><p>rdd1 = sc.parallelize([1, 2, 3, 4])<br>rdd2 = sc.parallelize([3, 4, 5, 6])<br>rdd1. intersection(rdd2).collect()<br>[3, 4]</p></blockquote><h5 id="（6）转换操作-distinct"><a href="#（6）转换操作-distinct" class="headerlink" title="（6）转换操作 distinct"></a>（6）转换操作 distinct</h5><p>列表转成集合（set(a_list)）， distinct在RDD中返回包含不同元素（会去重）的新RDD</p><blockquote><p>rdd2 = sc.parallelize([3, 4, 5, 6, 3, 4])<br>rdd2.distinct().collect()<br>[3, 4, 5, 6]</p></blockquote><h5 id="（7）转换操作-sortBy"><a href="#（7）转换操作-sortBy" class="headerlink" title="（7）转换操作 sortBy"></a>（7）转换操作 sortBy</h5><p>对一个RDD和根据指定的key进行排序，返回一个新的RDD，默认是升序排列，如果要降序排列，则添加参数ascending=False</p><blockquote><p>tmp = [(“a”, 1), (“b”, 2), (“1”, 3), (“d”, 4), (“2”, 5)]<br>sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()<br>[(‘1’, 3), (‘2’, 5), (‘a’, 1), (‘b’, 2), (‘d’, 4)]<br>sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()<br>[(‘a’, 1), (‘b’, 2), (‘1’, 3), (‘d’, 4), (‘2’, 5)]<br>sc.parallelize(tmp).sortBy(lambda x: x[1], ascending=False).collect()<br>[(‘2’, 5), (‘d’, 4), (‘1’, 3), (‘b’, 2), (‘a’, 1)]</p></blockquote><h5 id="（8）转换操作-glom"><a href="#（8）转换操作-glom" class="headerlink" title="（8）转换操作 glom"></a>（8）转换操作 glom</h5><p>将原RDD中相同分区的元素合并到同一个列表中，合并形成的列表就是新RDD中的一个元素。通过glom转换，可以知道原RDD分区的情况</p><blockquote><p>data = [1, 2, 3, 4, 5, 6, 7, 8, 9]<br>rdd = sc.parallelize(data, 4)<br>rdd = rdd.glom()<br>rdd.collect()<br>[[1, 2], [3, 4], [5, 6], [7, 8, 9]]</p></blockquote><h4 id="RDD的持久化"><a href="#RDD的持久化" class="headerlink" title="RDD的持久化"></a>RDD的持久化</h4><p>RDD的持久化可以使用persist方法和cache方法，cache方法只能缓存在内存中， persist方法可以缓存在磁盘上或者内存中。is_cached属性可以查看当前RDD的持久化状态，或者使用getStorageLevel方法获取当前RDD的持久化状态，unpersist方法可以解除RDD的持久化</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.storagelevel <span class="keyword">import</span> StorageLevel  <span class="comment"># 必须先引入StorageLevel这个类</span></span><br><span class="line">rdd = sc.parallelize([<span class="string">&quot;b&quot;</span>, <span class="string">&quot;a&quot;</span>, <span class="string">&quot;c&quot;</span>])</span><br><span class="line">rdd.persist(StorageLevel.MEMORY_ONLY)  <span class="comment"># 使用persist方法将RDD持久化到内存中</span></span><br><span class="line">ParallelCollectionRDD[<span class="number">27</span>] at parallelize at PythonRDD.scala:<span class="number">194</span> </span><br><span class="line">rdd.is_cached  <span class="comment"># 查看RDD的持久化状态：True</span></span><br><span class="line">rdd.unpersist() <span class="comment"># 解除RDD的持久化</span></span><br><span class="line">ParallelCollectionRDD[<span class="number">27</span>] at parallelize at PythonRDD.scala:<span class="number">194</span></span><br><span class="line">rdd.is_cached  <span class="comment"># 查看RDD的持久化状态：False</span></span><br><span class="line">rdd.persist(StorageLevel.DISK_ONLY)    <span class="comment"># 使用persist方法将RDD持久化到磁盘上</span></span><br><span class="line">ParallelCollectionRDD[<span class="number">27</span>] at parallelize at PythonRDD.scala:<span class="number">194</span></span><br><span class="line">rdd.getStorageLevel()  <span class="comment"># getStorageLevel方法获取当前RDD的持久化状态</span></span><br><span class="line">StorageLevel(<span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>检查点 checkpoint</p><p>通过cache或者persist将RDD持久化到内存或者磁盘中，这样做并不能保证数据完全不会丢失，当数据丢失的时候，Spark会根据RDD的计算流程DGA重新计算一遍，这样子就很费性能，checkpoint的作用就是将DAG中比较重要的中间数据做一个检查点将结果存储到一个高可用的地方(通常这个地方就是HDFS里面，当然也可以是本地文件系统)。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark概念</title>
      <link href="/2021/12/29/Spark%E6%A6%82%E5%BF%B5/"/>
      <url>/2021/12/29/Spark%E6%A6%82%E5%BF%B5/</url>
      
        <content type="html"><![CDATA[<h3 id="一、对spark有总体的认识，包括生态、架构、原理和特性等；"><a href="#一、对spark有总体的认识，包括生态、架构、原理和特性等；" class="headerlink" title="一、对spark有总体的认识，包括生态、架构、原理和特性等；"></a>一、对spark有总体的认识，包括生态、架构、原理和特性等；</h3><h4 id="1、Spark的概念："><a href="#1、Spark的概念：" class="headerlink" title="1、Spark的概念："></a>1、Spark的概念：</h4><p>Spark 是一个类于 Hadoop MapReduce 的通用并行框架, 由Scala语言实现的专门为大规模数据处理而设计的快速通用的技术分析引擎。</p><p>Spark的特点：快速、通用、易用、兼容性好。</p><p>Spark具备的能力：机器学习（ML库），实时流计算（Spark Streaming），SQL查询统计（Spark SQL），图表计算（GraphFrame）。</p><h4 id="2、Spark与Hadoop的区别与联系："><a href="#2、Spark与Hadoop的区别与联系：" class="headerlink" title="2、Spark与Hadoop的区别与联系："></a>2、Spark与Hadoop的区别与联系：</h4><p>（1）解决问题的方式不一样（2）两者可合可分</p><p>Spark相比HadoopMapRedue的优势：（1） 中间结果输出（2）数据格式和内存布局（3）执行策略（4） 任务调度的开销</p><h4 id="3、Spark-的用途："><a href="#3、Spark-的用途：" class="headerlink" title="3、Spark 的用途："></a>3、Spark 的用途：</h4><p>（1）推荐系统（2）快速查询系统（3）实时日志处理（4）定制广告系统（5）用户图计算</p><h4 id="4、Spark的生态系统："><a href="#4、Spark的生态系统：" class="headerlink" title="4、Spark的生态系统："></a>4、Spark的生态系统：</h4><p><img src="https://gitee.com/liyiwen1013/picture-transmission/raw/master/img/image-20211228003837117.png" alt="image-20211228003837117"></p><p>以Spark Core 为核心，利用Standalone、YARN 和Mesos 等资源调度管理，完成应用程序分析与处理。</p><h5 id="ps：了解："><a href="#ps：了解：" class="headerlink" title="ps：了解："></a>ps：了解：</h5><p>Spark Core提供Spark最基础与最核心的功能，它的子框架包括Spark SQL，Spark Streaming，MLlib和GraphX。</p><p>Spark SQL是一种结构化的数据处理模块。它提供了一个称为DataFrame的编程抽象，也可以作为分布式SQL查询引擎。</p><p>Spark Streaming是Sprak API核心的一个超高通量的扩展，可以处理实时数据流并容错。</p><p>Mllib（Machine Learning Library）是Spark提供的可扩展的机器学习库，包含了一些通用的学习算法和工具，如分类、回归、聚类、协同过滤、降维，以及底层的优化原语等算法和工具。</p><p>GraphX在Graphs和Graph-parallel并行计算中是一个新的部分，GraphX是Spark上的分布式图形处理架构，可用于图表计算。</p><h4 id="5、Spark-的构架"><a href="#5、Spark-的构架" class="headerlink" title="5、Spark 的构架"></a>5、Spark 的构架</h4><p><img src="https://gitee.com/liyiwen1013/picture-transmission/raw/master/img/image-20211228003137424.png" alt="image-20211228003137424"></p><p>Driver APP：客户端驱动程序或者应用程序，用于将任务程序转换为RDD和DAG，并与Cluster Manager进行通信与调度；<br>Cluster Manager：Spark的集群管理器，主要负责资源的分配和管理；<br>Worker：Spark的工作节点，对于Spark应用程序而言，由Cluster Manager分配资源给Worker，获得资源的Worker将创建Executor，并将资源和任务进一步分配给Executor，然后同步资源信息给Cluster Manager；<br>Executor：Spark的任务执行单元，负责任务的执行以及和Worker、Driver App同步信息。</p><h4 id="6、Spark核心原理"><a href="#6、Spark核心原理" class="headerlink" title="6、Spark核心原理"></a>6、Spark核心原理</h4><p><img src="https://gitee.com/liyiwen1013/picture-transmission/raw/master/img/image-20211228003519253.png" alt="image-20211228003519253"></p><p>(1) 从代码构建DAG图。<br>(2) 将DAG划分为Stage。<br>(3) Stage生成作业。<br>(4) FinalStage提交任务集。<br>(5) TaskSets提交任务。<br>(6) Tasks执行任务。<br>(7) Results跟踪结果。</p><p>Spark 2.X</p><h3 id="判断题："><a href="#判断题：" class="headerlink" title="判断题："></a>判断题：</h3><p>1、不仅仅Spark是基于内存的计算技术，Hadoop也是基于内存的计算，基本上所有的技术都基于内存进行计算。Spark只是把计算过程中间的结果缓存在内存中。<br>2、Spark只是在逻辑回归测试时候速度比Hadoop快了100倍，其他算法不一定。</p><h3 id="二、掌握spark-RDD的概念、算子的作用和使用，不同共享变量的作用和使用、对于RDD的依赖关系要理解，知道持久化的方法以类型；"><a href="#二、掌握spark-RDD的概念、算子的作用和使用，不同共享变量的作用和使用、对于RDD的依赖关系要理解，知道持久化的方法以类型；" class="headerlink" title="二、掌握spark RDD的概念、算子的作用和使用，不同共享变量的作用和使用、对于RDD的依赖关系要理解，知道持久化的方法以类型；"></a>二、掌握spark RDD的概念、算子的作用和使用，不同共享变量的作用和使用、对于RDD的依赖关系要理解，知道持久化的方法以类型；</h3><h4 id="1、Spark-RDD的概念"><a href="#1、Spark-RDD的概念" class="headerlink" title="1、Spark RDD的概念"></a>1、Spark RDD的概念</h4><p>RDD是可扩展的弹性分布式数据集；是只读、分区且不变的数据集合；是Spark的基石，也是Spark的灵魂；是一种分布式的内存抽象，不具备Schema的数据结构</p><p>RDD的五个主要属性：（1）分区信息（Partition)（2）自定义分片计算（3）RDD之间相互依赖（4）控制分片数量（5）使用列表方式进行块存储</p><h4 id="2、RDD算子"><a href="#2、RDD算子" class="headerlink" title="2、RDD算子"></a>2、RDD算子</h4><p>作用于RDD上的Operation分为转换(transformantion)和动作(action)。</p><p>RDD拥有的操作比MR丰富的多，不仅仅包括Map、Reduce操作，还包括filter、sort、join、save、count等操作，所以Spark比MR更容易方便完成更复杂的任务。</p><h4 id="3、RDD的依赖关系"><a href="#3、RDD的依赖关系" class="headerlink" title="3、RDD的依赖关系"></a>3、RDD的依赖关系</h4><p>RDD只能基于在稳定物理存储中的数据集和其他已有的RDD上执行确定性操作来创建。</p><h4 id="4、RDD持久化"><a href="#4、RDD持久化" class="headerlink" title="4、RDD持久化"></a>4、RDD持久化</h4><p> RDD的持久化可以使用persist方法和cache方法，cache方法只能缓存在内存中， persist方法可以缓存在磁盘上或者内存中。</p><h3 id="三、掌握对spark-dataframe和spark-sql的认识和使用（包括创建、各种常用操作，具体到代码的编写使用）；"><a href="#三、掌握对spark-dataframe和spark-sql的认识和使用（包括创建、各种常用操作，具体到代码的编写使用）；" class="headerlink" title="三、掌握对spark dataframe和spark sql的认识和使用（包括创建、各种常用操作，具体到代码的编写使用）；"></a>三、掌握对spark dataframe和spark sql的认识和使用（包括创建、各种常用操作，具体到代码的编写使用）；</h3><h4 id="1、Spark-DataFrame"><a href="#1、Spark-DataFrame" class="headerlink" title="1、Spark DataFrame"></a>1、Spark DataFrame</h4><p>在Spark中，Spark DataFrame和Spark SQL是SparkRDD高层次的封装，Spark DataFrame以RDD为基础，是一种与传统数据库中的二维表格相类似的分布式数据集。</p><h3 id="四、掌握spark-streaming的工作原理、离散化流、实时数据获取（套接字和文件夹）等内容，掌握Dstream的各种转换（具体到代码的编写使用）；"><a href="#四、掌握spark-streaming的工作原理、离散化流、实时数据获取（套接字和文件夹）等内容，掌握Dstream的各种转换（具体到代码的编写使用）；" class="headerlink" title="四、掌握spark streaming的工作原理、离散化流、实时数据获取（套接字和文件夹）等内容，掌握Dstream的各种转换（具体到代码的编写使用）；"></a>四、掌握spark streaming的工作原理、离散化流、实时数据获取（套接字和文件夹）等内容，掌握Dstream的各种转换（具体到代码的编写使用）；</h3><h4 id="1、流数据"><a href="#1、流数据" class="headerlink" title="1、流数据"></a>1、流数据</h4><p>流数据是一组顺序、大量、快速、连续到达的数据序列,一般情况下,数据流可被视为一个随时间延续而无限增长的动态数据集合。</p><h4 id="2、Spark-Streaming介绍"><a href="#2、Spark-Streaming介绍" class="headerlink" title="2、Spark Streaming介绍"></a>2、Spark Streaming介绍</h4><p>Spark Streaming 使得构建可扩展的容错流应用变得容易。它是Spark核心API的一个扩展，除了对实时性要求非常高（如高频实时交易）的计算场景，它能够满足所有的流式准实时计算场景。</p><h4 id="3、SparkStreaming工作原理——分批次进行处理"><a href="#3、SparkStreaming工作原理——分批次进行处理" class="headerlink" title="3、SparkStreaming工作原理——分批次进行处理"></a>3、SparkStreaming工作原理——分批次进行处理</h4><p>先接收实时输入的数据流，然后将数据拆分成多个batch（批），比如每收集1秒的数据封装为一个batch，然后将每个batch交给Spark的计算引擎进行处理，最后会生产出一个结果数据流，结果中的数据也是由一个一个的batch所组成的。</p><h4 id="4、离散流"><a href="#4、离散流" class="headerlink" title="4、离散流"></a>4、离散流</h4><p>离散流（discretized stream）简称“Dstream”，这是Spark Streaming对内部持续的实时数据流的抽象描述，也就是我们处理的一个实时数据流，在Spark Streaming中对应于一个DStream 实例。</p><h3 id="五、掌握spark-在机器学习的应用，包括MLlib（简单了解）和ML（重点掌握），对机器学习的整个流程要理解，对ML的转换器、预测器和管道要理解并知道如何在实际例子中使用，掌握管道和模型保存和加载的方法，对模型的评估指标有大概的认识理解；"><a href="#五、掌握spark-在机器学习的应用，包括MLlib（简单了解）和ML（重点掌握），对机器学习的整个流程要理解，对ML的转换器、预测器和管道要理解并知道如何在实际例子中使用，掌握管道和模型保存和加载的方法，对模型的评估指标有大概的认识理解；" class="headerlink" title="五、掌握spark 在机器学习的应用，包括MLlib（简单了解）和ML（重点掌握），对机器学习的整个流程要理解，对ML的转换器、预测器和管道要理解并知道如何在实际例子中使用，掌握管道和模型保存和加载的方法，对模型的评估指标有大概的认识理解；"></a>五、掌握spark 在机器学习的应用，包括MLlib（简单了解）和ML（重点掌握），对机器学习的整个流程要理解，对ML的转换器、预测器和管道要理解并知道如何在实际例子中使用，掌握管道和模型保存和加载的方法，对模型的评估指标有大概的认识理解；</h3><h4 id="1、机器学习的流程"><a href="#1、机器学习的流程" class="headerlink" title="1、机器学习的流程"></a>1、机器学习的流程</h4><p>机器学习的一般步骤是数据的采集，数据的加载，数据的探索，数据的预处理，训练模型，模型评估，模型的保存与调用。</p><h3 id="六、对图相关的概念要理解，掌握GraphFrames的使用以及实现的经典算法。"><a href="#六、对图相关的概念要理解，掌握GraphFrames的使用以及实现的经典算法。" class="headerlink" title="六、对图相关的概念要理解，掌握GraphFrames的使用以及实现的经典算法。"></a>六、对图相关的概念要理解，掌握GraphFrames的使用以及实现的经典算法。</h3><h4 id="1、图概念"><a href="#1、图概念" class="headerlink" title="1、图概念"></a>1、图概念</h4><p>在计算机科学中，图是一种重要的数据结构，它具有强大的表达能力，广泛应用于通信网络、搜索引擎、社交网络及自然语言处理等领域。</p><p>一般地，图(Graph)是由顶点的非空有限集和边的有限集构成的，记作G=<V,E>，其中G表示一个图，V表示图G中顶点(vertices)的集合，E表示是图G中边(edges)的集合，E中的边连接V中的两个顶点。</p>]]></content>
      
      
      <categories>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2021/12/05/hello-world/"/>
      <url>/2021/12/05/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
