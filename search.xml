<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Spark-RDD</title>
      <link href="/2021/12/29/Spark-RDD/"/>
      <url>/2021/12/29/Spark-RDD/</url>
      
        <content type="html"><![CDATA[<h3 id="二、掌握spark-RDD的概念、算子的作用和使用（包括创建和各种转换运算，具体到代码的编写使用），不同共享变量的作用和使用、对于RDD的依赖关系要理解，知道持久化的方法以及类型；"><a href="#二、掌握spark-RDD的概念、算子的作用和使用（包括创建和各种转换运算，具体到代码的编写使用），不同共享变量的作用和使用、对于RDD的依赖关系要理解，知道持久化的方法以及类型；" class="headerlink" title="二、掌握spark RDD的概念、算子的作用和使用（包括创建和各种转换运算，具体到代码的编写使用），不同共享变量的作用和使用、对于RDD的依赖关系要理解，知道持久化的方法以及类型；"></a>二、掌握spark RDD的概念、算子的作用和使用（包括创建和各种转换运算，具体到代码的编写使用），不同共享变量的作用和使用、对于RDD的依赖关系要理解，知道持久化的方法以及类型；</h3><h4 id="1、Spark-RDD的概念"><a href="#1、Spark-RDD的概念" class="headerlink" title="1、Spark RDD的概念"></a>1、Spark RDD的概念</h4><p>RDD是可扩展的弹性分布式数据集（一种容错的并行数据结构）；是只读、分区且不变的数据集合；是Spark的基石，也是Spark的灵魂；是一种分布式的内存抽象，不具备Schema的数据结构（可以基于任何数据结构创建，如tuple（元组）、dict（字典）和list（列表））</p><p>RDD的五个主要属性：（1）分区信息（Partition)（2）自定义分片计算（3）RDD之间相互依赖（4）控制分片数量（5）使用列表方式进行块存储</p><h4 id="2、创建RDD"><a href="#2、创建RDD" class="headerlink" title="2、创建RDD"></a>2、创建RDD</h4><h5 id="（1）基于数据集合创建RDD"><a href="#（1）基于数据集合创建RDD" class="headerlink" title="（1）基于数据集合创建RDD"></a>（1）基于数据集合创建RDD</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">distData1 = sc.parallelize(data)   <span class="comment"># parallelize函数创建RDD</span></span><br><span class="line">distData2 = sc.parallelize(data, <span class="number">2</span>)  <span class="comment"># 指定分区数创建RDD，指定2个分区</span></span><br><span class="line">distData2.count() <span class="comment"># count函数返回RDD中元素的个数（不是某个分区的，是全部的元素个数）。</span></span><br></pre></td></tr></table></figure><p>遍历：对list、tuple、dict、set、str等类型的数据使用for…in…的循环语法从其中依次拿到数据进行使用，这样的过程称为遍历，也叫迭代。</p><p>可迭代对象（Iterable）：通过for…in…这类语句迭代读取一条数据供我们使用的对象。</p><h5 id="（2）基于外部数据源创建RDD"><a href="#（2）基于外部数据源创建RDD" class="headerlink" title="（2）基于外部数据源创建RDD"></a>（2）基于外部数据源创建RDD</h5><p>textFile 函数可以将一个外部数据源转换为RDD对象，文件的一行数据就是RDD的一个元素，需要传入该数据源的url，</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">distFile1 = sc.textFile(“file:///home/camel/Repos/spark/README.md”) <span class="comment"># 本地文件</span></span><br><span class="line">distFile2 = sc.textFile(“hdfs://地址：端口号/test/output”) <span class="comment"># hdfs上的文件</span></span><br><span class="line">distFile1.count() <span class="comment"># 此时的count函数返回的是文本的行数</span></span><br></pre></td></tr></table></figure><h4 id="3、RDD算子"><a href="#3、RDD算子" class="headerlink" title="3、RDD算子"></a>3、RDD算子</h4><p>作用于RDD上的Operation分为转换(transformantion)和动作(action)。</p><p>RDD拥有的操作比MR丰富的多，不仅仅包括Map、Reduce操作，还包括filter、sort、join、save、count等操作，所以Spark比MR更容易方便完成更复杂的任务。</p><h5 id="（1）转换操作-map"><a href="#（1）转换操作-map" class="headerlink" title="（1）转换操作 map"></a>（1）转换操作 map</h5><p>将指定的函数作用在RDD的每个分区的每个元素上</p><blockquote><p>rdd = sc.parallelize([“b”, “a”, “c”])<br>rdd.map(lambda x: (x, 1)).collect()<br>结果：<br>[(‘b’, 1), (‘a’, 1), (‘c’, 1)]</p></blockquote><p>map会依次取出rdd中的每一个元素，然后传给lambda x: (x, 1)中x变量，接着lambda针对x变量进行生成tuple (x,1)的操作，collect()是将各内存中的结果返回到驱动端，返回的是包含RDD所有元素的列表（list）</p><h5 id="（2）转换操作-flatMap"><a href="#（2）转换操作-flatMap" class="headerlink" title="（2）转换操作 flatMap"></a>（2）转换操作 flatMap</h5><p>首先将map函数应用于该RDD的所有元素，然后将结果平坦化（可以理解为将类型为元祖的元素逐个放出来），从而返回新的RDD</p><blockquote><p>rdd = sc.parallelize([2, 3, 4])<br>rdd.flatMap(lambda x: [(x, x), (x, x)]).collect()<br>[(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]</p><p>如果将flatMap换成map，结果则为：<br>[[(2, 2), (2, 2)], [(3, 3), (3, 3)], [(4, 4), (4, 4)]]</p></blockquote><h5 id="（3）转换操作-filter"><a href="#（3）转换操作-filter" class="headerlink" title="（3）转换操作 filter"></a>（3）转换操作 filter</h5><p>用指定的函数作为过滤条件，在RDD的所有分区中筛选出符合条件的元素</p><blockquote><p>rdd = sc.parallelize([1, 2, 3, 4, 5])<br>rdd.filter(lambda x: x % 2 == 0).collect()<br>[2, 4]</p></blockquote><h5 id="（4）转换操作-union"><a href="#（4）转换操作-union" class="headerlink" title="（4）转换操作 union"></a>（4）转换操作 union</h5><p>对一个RDD和参数RDD求并集后，返回一个新的RDD，只不过这里的union不会去重</p><blockquote><p>rdd1 = sc.parallelize([1, 2, 3, 4])<br>rdd2 = sc.parallelize([3, 4, 5, 6])<br>rdd1.union(rdd2).collect()<br>[1, 2, 3, 4, 3, 4, 5, 6]</p></blockquote><h5 id="（5）转换操作-intersection"><a href="#（5）转换操作-intersection" class="headerlink" title="（5）转换操作 intersection"></a>（5）转换操作 intersection</h5><p>对一个RDD和参数RDD求交集后，返回一个新的RDD</p><blockquote><p>rdd1 = sc.parallelize([1, 2, 3, 4])<br>rdd2 = sc.parallelize([3, 4, 5, 6])<br>rdd1. intersection(rdd2).collect()<br>[3, 4]</p></blockquote><h5 id="（6）转换操作-distinct"><a href="#（6）转换操作-distinct" class="headerlink" title="（6）转换操作 distinct"></a>（6）转换操作 distinct</h5><p>列表转成集合（set(a_list)）， distinct在RDD中返回包含不同元素（会去重）的新RDD</p><blockquote><p>rdd2 = sc.parallelize([3, 4, 5, 6, 3, 4])<br>rdd2.distinct().collect()<br>[3, 4, 5, 6]</p></blockquote><h5 id="（7）转换操作-sortBy"><a href="#（7）转换操作-sortBy" class="headerlink" title="（7）转换操作 sortBy"></a>（7）转换操作 sortBy</h5><p>对一个RDD和根据指定的key进行排序，返回一个新的RDD，默认是升序排列，如果要降序排列，则添加参数ascending=False</p><blockquote><p>tmp = [(“a”, 1), (“b”, 2), (“1”, 3), (“d”, 4), (“2”, 5)]<br>sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()<br>[(‘1’, 3), (‘2’, 5), (‘a’, 1), (‘b’, 2), (‘d’, 4)]<br>sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()<br>[(‘a’, 1), (‘b’, 2), (‘1’, 3), (‘d’, 4), (‘2’, 5)]<br>sc.parallelize(tmp).sortBy(lambda x: x[1], ascending=False).collect()<br>[(‘2’, 5), (‘d’, 4), (‘1’, 3), (‘b’, 2), (‘a’, 1)]</p></blockquote><h5 id="（8）转换操作-glom"><a href="#（8）转换操作-glom" class="headerlink" title="（8）转换操作 glom"></a>（8）转换操作 glom</h5><p>将原RDD中相同分区的元素合并到同一个列表中，合并形成的列表就是新RDD中的一个元素。通过glom转换，可以知道原RDD分区的情况</p><blockquote><p>data = [1, 2, 3, 4, 5, 6, 7, 8, 9]<br>rdd = sc.parallelize(data, 4)<br>rdd = rdd.glom()<br>rdd.collect()<br>[[1, 2], [3, 4], [5, 6], [7, 8, 9]]</p></blockquote><h4 id="RDD的持久化"><a href="#RDD的持久化" class="headerlink" title="RDD的持久化"></a>RDD的持久化</h4><p>RDD的持久化可以使用persist方法和cache方法，cache方法只能缓存在内存中， persist方法可以缓存在磁盘上或者内存中。is_cached属性可以查看当前RDD的持久化状态，或者使用getStorageLevel方法获取当前RDD的持久化状态，unpersist方法可以解除RDD的持久化</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.storagelevel <span class="keyword">import</span> StorageLevel  <span class="comment"># 必须先引入StorageLevel这个类</span></span><br><span class="line">rdd = sc.parallelize([<span class="string">&quot;b&quot;</span>, <span class="string">&quot;a&quot;</span>, <span class="string">&quot;c&quot;</span>])</span><br><span class="line">rdd.persist(StorageLevel.MEMORY_ONLY)  <span class="comment"># 使用persist方法将RDD持久化到内存中</span></span><br><span class="line">ParallelCollectionRDD[<span class="number">27</span>] at parallelize at PythonRDD.scala:<span class="number">194</span> </span><br><span class="line">rdd.is_cached  <span class="comment"># 查看RDD的持久化状态：True</span></span><br><span class="line">rdd.unpersist() <span class="comment"># 解除RDD的持久化</span></span><br><span class="line">ParallelCollectionRDD[<span class="number">27</span>] at parallelize at PythonRDD.scala:<span class="number">194</span></span><br><span class="line">rdd.is_cached  <span class="comment"># 查看RDD的持久化状态：False</span></span><br><span class="line">rdd.persist(StorageLevel.DISK_ONLY)    <span class="comment"># 使用persist方法将RDD持久化到磁盘上</span></span><br><span class="line">ParallelCollectionRDD[<span class="number">27</span>] at parallelize at PythonRDD.scala:<span class="number">194</span></span><br><span class="line">rdd.getStorageLevel()  <span class="comment"># getStorageLevel方法获取当前RDD的持久化状态</span></span><br><span class="line">StorageLevel(<span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>检查点 checkpoint</p><p>通过cache或者persist将RDD持久化到内存或者磁盘中，这样做并不能保证数据完全不会丢失，当数据丢失的时候，Spark会根据RDD的计算流程DGA重新计算一遍，这样子就很费性能，checkpoint的作用就是将DAG中比较重要的中间数据做一个检查点将结果存储到一个高可用的地方(通常这个地方就是HDFS里面，当然也可以是本地文件系统)。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Spark概念</title>
      <link href="/2021/12/29/Spark%E6%A6%82%E5%BF%B5/"/>
      <url>/2021/12/29/Spark%E6%A6%82%E5%BF%B5/</url>
      
        <content type="html"><![CDATA[<h2 id="spark期末考试题型和分值设置"><a href="#spark期末考试题型和分值设置" class="headerlink" title="spark期末考试题型和分值设置"></a>spark期末考试题型和分值设置</h2><p>选择题  12题，每题  2分，共24分<br>填空题  10空，每空  1分，共10分<br>判断题  10题，每题  1分，共10分<br>程序题    6题，每题  6分，共36分<br>简答题    3题，8/ 6/ 6分，共20分</p><p>题目的设计参考了如下的能力要求：</p><h2 id="Spark课程能力要求："><a href="#Spark课程能力要求：" class="headerlink" title="Spark课程能力要求："></a>Spark课程能力要求：</h2><h3 id="一、对spark有总体的认识，包括生态、架构、原理和特性等；"><a href="#一、对spark有总体的认识，包括生态、架构、原理和特性等；" class="headerlink" title="一、对spark有总体的认识，包括生态、架构、原理和特性等；"></a>一、对spark有总体的认识，包括生态、架构、原理和特性等；</h3><h4 id="1、Spark的概念："><a href="#1、Spark的概念：" class="headerlink" title="1、Spark的概念："></a>1、Spark的概念：</h4><p>Spark 是一个类于 Hadoop MapReduce 的通用并行框架, 由Scala语言实现的专门为大规模数据处理而设计的快速通用的技术分析引擎。</p><p>Spark的特点：快速、通用、易用、兼容性好。</p><p>Spark具备的能力：机器学习（ML库），实时流计算（Spark Streaming），SQL查询统计（Spark SQL），图表计算（GraphFrame）。</p><h4 id="2、Spark与Hadoop的区别与联系："><a href="#2、Spark与Hadoop的区别与联系：" class="headerlink" title="2、Spark与Hadoop的区别与联系："></a>2、Spark与Hadoop的区别与联系：</h4><p>（1）解决问题的方式不一样（2）两者可合可分</p><p>Spark相比HadoopMapRedue的优势：（1） 中间结果输出（2）数据格式和内存布局（3）执行策略（4） 任务调度的开销</p><h4 id="3、Spark-的用途："><a href="#3、Spark-的用途：" class="headerlink" title="3、Spark 的用途："></a>3、Spark 的用途：</h4><p>（1）推荐系统（2）快速查询系统（3）实时日志处理（4）定制广告系统（5）用户图计算</p><h4 id="4、Spark的生态系统："><a href="#4、Spark的生态系统：" class="headerlink" title="4、Spark的生态系统："></a>4、Spark的生态系统：</h4><p><img src="https://gitee.com/liyiwen1013/picture-transmission/raw/master/img/image-20211228003837117.png" alt="image-20211228003837117"></p><p>以Spark Core 为核心，利用Standalone、YARN 和Mesos 等资源调度管理，完成应用程序分析与处理。</p><h5 id="ps：了解："><a href="#ps：了解：" class="headerlink" title="ps：了解："></a>ps：了解：</h5><p>Spark Core提供Spark最基础与最核心的功能，它的子框架包括Spark SQL，Spark Streaming，MLlib和GraphX。</p><p>Spark SQL是一种结构化的数据处理模块。它提供了一个称为DataFrame的编程抽象，也可以作为分布式SQL查询引擎。</p><p>Spark Streaming是Sprak API核心的一个超高通量的扩展，可以处理实时数据流并容错。</p><p>Mllib（Machine Learning Library）是Spark提供的可扩展的机器学习库，包含了一些通用的学习算法和工具，如分类、回归、聚类、协同过滤、降维，以及底层的优化原语等算法和工具。</p><p>GraphX在Graphs和Graph-parallel并行计算中是一个新的部分，GraphX是Spark上的分布式图形处理架构，可用于图表计算。</p><h4 id="5、Spark-的构架"><a href="#5、Spark-的构架" class="headerlink" title="5、Spark 的构架"></a>5、Spark 的构架</h4><p><img src="https://gitee.com/liyiwen1013/picture-transmission/raw/master/img/image-20211228003137424.png" alt="image-20211228003137424"></p><p>Driver APP：客户端驱动程序或者应用程序，用于将任务程序转换为RDD和DAG，并与Cluster Manager进行通信与调度；<br>Cluster Manager：Spark的集群管理器，主要负责资源的分配和管理；<br>Worker：Spark的工作节点，对于Spark应用程序而言，由Cluster Manager分配资源给Worker，获得资源的Worker将创建Executor，并将资源和任务进一步分配给Executor，然后同步资源信息给Cluster Manager；<br>Executor：Spark的任务执行单元，负责任务的执行以及和Worker、Driver App同步信息。</p><h4 id="6、Spark核心原理"><a href="#6、Spark核心原理" class="headerlink" title="6、Spark核心原理"></a>6、Spark核心原理</h4><p><img src="https://gitee.com/liyiwen1013/picture-transmission/raw/master/img/image-20211228003519253.png" alt="image-20211228003519253"></p><p>(1) 从代码构建DAG图。<br>(2) 将DAG划分为Stage。<br>(3) Stage生成作业。<br>(4) FinalStage提交任务集。<br>(5) TaskSets提交任务。<br>(6) Tasks执行任务。<br>(7) Results跟踪结果。</p><p>Spark 2.X</p><h3 id="判断题："><a href="#判断题：" class="headerlink" title="判断题："></a>判断题：</h3><p>1、不仅仅Spark是基于内存的计算技术，Hadoop也是基于内存的计算，基本上所有的技术都基于内存进行计算。Spark只是把计算过程中间的结果缓存在内存中。<br>2、Spark只是在逻辑回归测试时候速度比Hadoop快了100倍，其他算法不一定。</p><h3 id="二、掌握spark-RDD的概念、算子的作用和使用，不同共享变量的作用和使用、对于RDD的依赖关系要理解，知道持久化的方法以类型；"><a href="#二、掌握spark-RDD的概念、算子的作用和使用，不同共享变量的作用和使用、对于RDD的依赖关系要理解，知道持久化的方法以类型；" class="headerlink" title="二、掌握spark RDD的概念、算子的作用和使用，不同共享变量的作用和使用、对于RDD的依赖关系要理解，知道持久化的方法以类型；"></a>二、掌握spark RDD的概念、算子的作用和使用，不同共享变量的作用和使用、对于RDD的依赖关系要理解，知道持久化的方法以类型；</h3><h4 id="1、Spark-RDD的概念"><a href="#1、Spark-RDD的概念" class="headerlink" title="1、Spark RDD的概念"></a>1、Spark RDD的概念</h4><p>RDD是可扩展的弹性分布式数据集；是只读、分区且不变的数据集合；是Spark的基石，也是Spark的灵魂；是一种分布式的内存抽象，不具备Schema的数据结构</p><p>RDD的五个主要属性：（1）分区信息（Partition)（2）自定义分片计算（3）RDD之间相互依赖（4）控制分片数量（5）使用列表方式进行块存储</p><h4 id="2、RDD算子"><a href="#2、RDD算子" class="headerlink" title="2、RDD算子"></a>2、RDD算子</h4><p>作用于RDD上的Operation分为转换(transformantion)和动作(action)。</p><p>RDD拥有的操作比MR丰富的多，不仅仅包括Map、Reduce操作，还包括filter、sort、join、save、count等操作，所以Spark比MR更容易方便完成更复杂的任务。</p><h4 id="3、RDD的依赖关系"><a href="#3、RDD的依赖关系" class="headerlink" title="3、RDD的依赖关系"></a>3、RDD的依赖关系</h4><p>RDD只能基于在稳定物理存储中的数据集和其他已有的RDD上执行确定性操作来创建。</p><h4 id="4、RDD持久化"><a href="#4、RDD持久化" class="headerlink" title="4、RDD持久化"></a>4、RDD持久化</h4><p> RDD的持久化可以使用persist方法和cache方法，cache方法只能缓存在内存中， persist方法可以缓存在磁盘上或者内存中。</p><h3 id="三、掌握对spark-dataframe和spark-sql的认识和使用（包括创建、各种常用操作，具体到代码的编写使用）；"><a href="#三、掌握对spark-dataframe和spark-sql的认识和使用（包括创建、各种常用操作，具体到代码的编写使用）；" class="headerlink" title="三、掌握对spark dataframe和spark sql的认识和使用（包括创建、各种常用操作，具体到代码的编写使用）；"></a>三、掌握对spark dataframe和spark sql的认识和使用（包括创建、各种常用操作，具体到代码的编写使用）；</h3><h4 id="1、Spark-DataFrame"><a href="#1、Spark-DataFrame" class="headerlink" title="1、Spark DataFrame"></a>1、Spark DataFrame</h4><p>在Spark中，Spark DataFrame和Spark SQL是SparkRDD高层次的封装，Spark DataFrame以RDD为基础，是一种与传统数据库中的二维表格相类似的分布式数据集。</p><h3 id="四、掌握spark-streaming的工作原理、离散化流、实时数据获取（套接字和文件夹）等内容，掌握Dstream的各种转换（具体到代码的编写使用）；"><a href="#四、掌握spark-streaming的工作原理、离散化流、实时数据获取（套接字和文件夹）等内容，掌握Dstream的各种转换（具体到代码的编写使用）；" class="headerlink" title="四、掌握spark streaming的工作原理、离散化流、实时数据获取（套接字和文件夹）等内容，掌握Dstream的各种转换（具体到代码的编写使用）；"></a>四、掌握spark streaming的工作原理、离散化流、实时数据获取（套接字和文件夹）等内容，掌握Dstream的各种转换（具体到代码的编写使用）；</h3><h4 id="1、流数据"><a href="#1、流数据" class="headerlink" title="1、流数据"></a>1、流数据</h4><p>流数据是一组顺序、大量、快速、连续到达的数据序列,一般情况下,数据流可被视为一个随时间延续而无限增长的动态数据集合。</p><h4 id="2、Spark-Streaming介绍"><a href="#2、Spark-Streaming介绍" class="headerlink" title="2、Spark Streaming介绍"></a>2、Spark Streaming介绍</h4><p>Spark Streaming 使得构建可扩展的容错流应用变得容易。它是Spark核心API的一个扩展，除了对实时性要求非常高（如高频实时交易）的计算场景，它能够满足所有的流式准实时计算场景。</p><h4 id="3、SparkStreaming工作原理——分批次进行处理"><a href="#3、SparkStreaming工作原理——分批次进行处理" class="headerlink" title="3、SparkStreaming工作原理——分批次进行处理"></a>3、SparkStreaming工作原理——分批次进行处理</h4><p>先接收实时输入的数据流，然后将数据拆分成多个batch（批），比如每收集1秒的数据封装为一个batch，然后将每个batch交给Spark的计算引擎进行处理，最后会生产出一个结果数据流，结果中的数据也是由一个一个的batch所组成的。</p><h4 id="4、离散流"><a href="#4、离散流" class="headerlink" title="4、离散流"></a>4、离散流</h4><p>离散流（discretized stream）简称“Dstream”，这是Spark Streaming对内部持续的实时数据流的抽象描述，也就是我们处理的一个实时数据流，在Spark Streaming中对应于一个DStream 实例。</p><h3 id="五、掌握spark-在机器学习的应用，包括MLlib（简单了解）和ML（重点掌握），对机器学习的整个流程要理解，对ML的转换器、预测器和管道要理解并知道如何在实际例子中使用，掌握管道和模型保存和加载的方法，对模型的评估指标有大概的认识理解；"><a href="#五、掌握spark-在机器学习的应用，包括MLlib（简单了解）和ML（重点掌握），对机器学习的整个流程要理解，对ML的转换器、预测器和管道要理解并知道如何在实际例子中使用，掌握管道和模型保存和加载的方法，对模型的评估指标有大概的认识理解；" class="headerlink" title="五、掌握spark 在机器学习的应用，包括MLlib（简单了解）和ML（重点掌握），对机器学习的整个流程要理解，对ML的转换器、预测器和管道要理解并知道如何在实际例子中使用，掌握管道和模型保存和加载的方法，对模型的评估指标有大概的认识理解；"></a>五、掌握spark 在机器学习的应用，包括MLlib（简单了解）和ML（重点掌握），对机器学习的整个流程要理解，对ML的转换器、预测器和管道要理解并知道如何在实际例子中使用，掌握管道和模型保存和加载的方法，对模型的评估指标有大概的认识理解；</h3><h4 id="1、机器学习的流程"><a href="#1、机器学习的流程" class="headerlink" title="1、机器学习的流程"></a>1、机器学习的流程</h4><p>机器学习的一般步骤是数据的采集，数据的加载，数据的探索，数据的预处理，训练模型，模型评估，模型的保存与调用。</p><h3 id="六、对图相关的概念要理解，掌握GraphFrames的使用以及实现的经典算法。"><a href="#六、对图相关的概念要理解，掌握GraphFrames的使用以及实现的经典算法。" class="headerlink" title="六、对图相关的概念要理解，掌握GraphFrames的使用以及实现的经典算法。"></a>六、对图相关的概念要理解，掌握GraphFrames的使用以及实现的经典算法。</h3><h4 id="1、图概念"><a href="#1、图概念" class="headerlink" title="1、图概念"></a>1、图概念</h4><p>在计算机科学中，图是一种重要的数据结构，它具有强大的表达能力，广泛应用于通信网络、搜索引擎、社交网络及自然语言处理等领域。</p><p>一般地，图(Graph)是由顶点的非空有限集和边的有限集构成的，记作G=&lt;V,E&gt;，其中G表示一个图，V表示图G中顶点(vertices)的集合，E表示是图G中边(edges)的集合，E中的边连接V中的两个顶点。</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法知识 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2021/12/05/hello-world/"/>
      <url>/2021/12/05/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
